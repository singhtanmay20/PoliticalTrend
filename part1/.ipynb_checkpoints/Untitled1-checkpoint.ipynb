{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "#import pandas as pd\n",
    "import re\n",
    "\n",
    "####input your credentials here\n",
    "consumer_key = 'Tjem3g5DeznzN7vmyaLTcRAAy'\n",
    "consumer_secret = 'ybi5t7oYZGzaRs9OlkXkFOaIbNjwI60RglrECeWr6cqhLUiZGd'\n",
    "access_token = '1100913179260080128-nzmIARZWuNmc4zxJteUkGqbbPCKZY0'\n",
    "access_token_secret = 'ZWtvqwcDavUNE1efzjxanDMEtIcUgeuFrhrSc4kUUPQ8I'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "#####United Airlines\n",
    "# Open/Create a file to append data\n",
    "csvFile = open('twittermusic2.txt', 'a')\n",
    "#Use csv Writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "\n",
    "print(\"entering tweepy\")\n",
    "originalTweets=[]\n",
    "for tweet in tweepy.Cursor(api.search,q=\"music -filter:retweets\",count=5000,\n",
    "                           lang=\"en\",\n",
    "                           since=\"2019-01-01\",retry_count = 5, \n",
    "                       retry_delay = 5).items():\n",
    "    #print (tweet.text)\n",
    "    if not tweet.retweeted:\n",
    "        if tweet.text not in originalTweets:\n",
    "            originalTweets.append(tweet.text)\n",
    "            Tweet = re.sub('@\\S+','',tweet.text)\n",
    "            Tweet1=re.sub(r'http\\S+', '', Tweet, flags=re.MULTILINE)\n",
    "            Tweet2=re.sub('#','',Tweet1)\n",
    "            print(Tweet2)\n",
    "            csvWriter.writerow([Tweet2.encode('ascii','ignore').decode('ascii')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYT Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nytimesarticle import articleAPI\n",
    "api = articleAPI('MqGpINkbKmy15GOA36p7DYm09JTXSAaO ')\n",
    "import json\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "all_articles = []\n",
    "\n",
    "def get_articles(query):\n",
    "    j=4\n",
    "    for i in range(1,11): \n",
    "        articles = api.search(q = query, begin_date=20190101,page=i)\n",
    "        print(\"-------------------------------------------------------------------------------------------\")\n",
    "        print(i)\n",
    "        if(\"response\" in articles):\n",
    "            \n",
    "            for items in articles[\"response\"][\"docs\"]:\n",
    "                f=open('nytentertainment'+str(j)+\".txt\",'w')\n",
    "                article_url=items[\"web_url\"]\n",
    "                soup = BeautifulSoup(urllib.request.urlopen(article_url), 'html.parser')\n",
    "                soup.prettify()\n",
    "                print(article_url)\n",
    "                # retrieve all of the paragraph tags\n",
    "                paragraphs=soup.find('article').find_all('p')\n",
    "                for paragraph in paragraphs:\n",
    "                    f.write(paragraph.text)\n",
    "                j+=1\n",
    "                \n",
    "                all_articles.append(article_url)\n",
    "            \n",
    "    return(all_articles)\n",
    "\n",
    "f=open('nyturlentertainment','w')\n",
    "Articles=get_articles(\"entertainment\")\n",
    "for i in range(len(Articles)):\n",
    "    f.write(Articles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common-Crawl Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import StringIO\n",
    "import gzip\n",
    "import csv\n",
    "import codecs\n",
    "import ast\n",
    "import zlib\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    import cStringIO as stringIOModule\n",
    "except ImportError:\n",
    "    try:\n",
    "        import StringIO as stringIOModule\n",
    "    except ImportError:\n",
    "        from io import StringIO as stringIOModule\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "if sys.version[0] == '2':\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "index_list = \"2019-09\"\n",
    "domain = \"www.washingtonpost.com/entertainment/\"\n",
    "\n",
    "def search_domain(domain):\n",
    "\n",
    "    record_list = []\n",
    "    for index in index_list:\n",
    "        \n",
    "        print(\"[*] Trying index %s\" % index_list)\n",
    "        \n",
    "        cc_url  = \"http://index.commoncrawl.org/CC-MAIN-%s-index?\" % index_list\n",
    "        cc_url += \"url=https://www.washingtonpost.com/entertainment/&matchType=domain&output=json\" \n",
    "        \n",
    "        response = requests.get(cc_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            records = response.content.splitlines()\n",
    "            \n",
    "            for record in records:\n",
    "                record_list.append(record.decode('utf-8'))\n",
    "            \n",
    "            print(\"[*] Added %d results.\" % len(records))\n",
    "            \n",
    "    \n",
    "    print(\"[*] Found a total of %d hits.\" % len(record_list))\n",
    "    \n",
    "    return record_list        \n",
    "\n",
    "def download_page(record):\n",
    "    \n",
    "    print(record[\"offset\"])\n",
    "    offset, length = int(record[\"offset\"]), int(record[\"length\"])\n",
    "    offset_end = offset + length - 1\n",
    "    prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
    "    print(\"--------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    # We can then use the Range header to ask for just this set of bytes\n",
    "    resp =  requests.get(prefix + record['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
    "    raw_data = StringIO.StringIO(resp.content)\n",
    "    f = gzip.GzipFile(fileobj=raw_data)\n",
    "    data = f.read()\n",
    "    response = \"\"\n",
    "    if len(data):\n",
    "        try:\n",
    "            warc, header, response = data.strip().split('\\r\\n', 2)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return response\n",
    "\n",
    "\n",
    "def extract_external_links(html_content,link_list):\n",
    "\n",
    "    parser = BeautifulSoup(html_content)\n",
    "        \n",
    "    links = parser.find_all(\"a\")\n",
    "    \n",
    "    if links:\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.attrs.get(\"href\")\n",
    "            \n",
    "            if href is not None:\n",
    "                if href.encode(\"utf-8\") not in link_list and href.startswith(\"http\"):\n",
    "                    print( \"[*] Discovered external link: %s\" % href)\n",
    "                    link_list.append(href.encode('utf-8'))\n",
    "\n",
    "    return link_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "record_list = search_domain(\"https://www.washingtonpost.com/entertainment/\")\n",
    "link_list   = []\n",
    "\n",
    "for record in record_list:\n",
    "    record=ast.literal_eval(record)\n",
    "    html_content = download_page(record)\n",
    "    \n",
    "    print(\"[*] Retrieved %d bytes for %s\" % (len(html_content),record['url']))\n",
    "    \n",
    "    link_list = extract_external_links(html_content,link_list)\n",
    "    with open(\"/home/tanmay/Documents/DIC/Proj2/ccurl1.txt\", 'a') as file:\n",
    "        for link in link_list:\n",
    "            file.write(link+\"\\n\")\n",
    "\n",
    "print(\"[*] Total external links discovered: %d\" % len(link_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
